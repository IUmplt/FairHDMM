{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f52b58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from src.hdmm.error import expected_error, strategy_supports_workload\n",
    "from src.hdmm.matrix import EkteloMatrix\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from src.hdmm.workload import AllRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cec2d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:0.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64eae0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBounds(df):\n",
    "    \"\"\"\n",
    "    Returns [upper bound error, lower bound error]\n",
    "    \"\"\"\n",
    "    return [df.abs_error.min(), df.abs_error.max()]\n",
    "\n",
    "def getAverageError(df):\n",
    "    \"\"\"\n",
    "    Returns average error across all queries\n",
    "    \"\"\"\n",
    "    return df.abs_error.sum() / len(df)\n",
    "\n",
    "def printBoundsAndAvgError(df):\n",
    "    print(f'Average error is {getAverageError(df)}. Lower bound is {getBounds(df)[0]} and upper bound is {getBounds(df)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77bd3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmw2(workload, x, T, eps=0.01, k=0, analyst_labels = [], \n",
    "         show_messages=True, to_return='pd', show_plot=False, show_failure_step=True):\n",
    "    \"\"\"\n",
    "    Implement Private Multiplicative Weights Mechanism (PMW) on a workload of\n",
    "    linear queries. \n",
    "\n",
    "    Algorithm Parameters: \n",
    "    - workload = workload of queries (M x k numpy array)\n",
    "    - x = true database (M x 1 numpy array)\n",
    "    - T = update threshold\n",
    "    - eps = privacy budget\n",
    "    - k = number of update steps PER ANALYST\n",
    "    - analyst_labels = list of analyst names corresponding to each query in the workload\n",
    "    \n",
    "    Output Controls: \n",
    "    - show_messages argument determines whether the function will print information such as \n",
    "    error scale, threshold, update steps used, etc.\n",
    "    - to_return argument determines what the function will return. \n",
    "        - if 'pd', pmw() returns pandas df with test data for each \n",
    "        query in the workload(showing query, d_t_hat, updated, algo_ans, real_ans, \n",
    "        abs_error, rel_error). \n",
    "        - if 'update_count', pmw() returns the update count for the total\n",
    "        amount of queries.\n",
    "    - show_plot - T/F whether the function will display a plot\n",
    "    - show_failure_step - T/F whether function prints what step failure mode is reached\n",
    "    \"\"\" \n",
    "    \n",
    "    update_steps = {}\n",
    "    for analyst in list(set(analyst_labels)): \n",
    "        update_steps[analyst] = k # each analyst starts with k update steps\n",
    "    \n",
    "    # initialize constants\n",
    "    m = x.size  # database len\n",
    "    n = x.sum()\n",
    "    eta = (math.log(m, np.e) ** (1 / 4)) / (math.sqrt(n))\n",
    "    delta = 1 / (n * math.log(n, np.e))\n",
    "    x_norm = x / np.sum(x)\n",
    "    \n",
    "    # initialize synthetic databases at time 0 (prior to any queries)\n",
    "    x_t = np.ones(m) / m\n",
    "    y_t = np.ones(m) / m\n",
    "\n",
    "    # initialize tracker lists to construct pandas dataframe at the end \n",
    "    x_list = [x_t] # create a list of x_t synthetic database at every time step\n",
    "    update_list = []\n",
    "    update_count = 0\n",
    "    pmw_answers = []\n",
    "    update_times = [] # record times that database is updated\n",
    "    d_t_hat_list = []\n",
    "    \n",
    "    def lazy_round():\n",
    "        \"\"\"\n",
    "        \"Lazy Round\" of querying using the stored synthetic database, x_t, in list x_list.\n",
    "        \n",
    "        We call this the lazy round because it is contrasted with the updated step where we update the \n",
    "        sythetic database and answer the query using the real database.\n",
    "        \"\"\"\n",
    "        update_list.append('no')\n",
    "        pmw_answers.append(np.dot(query, x_list[time]))\n",
    "        x_list.append(x_list[time].round(3))\n",
    "    \n",
    "    # inititate first instance of SVT with half the budget and k updates; will be reset in the main loop\n",
    "    SVTtrigger = False \n",
    "    SVTepsilon1 = ((eps/2)/2)\n",
    "    SVTepsilon2 = ((eps/2)/2)\n",
    "    rho = np.random.laplace(loc=0, scale=(1/SVTepsilon1), size=1)[0]\n",
    "    \n",
    "    for time, query in enumerate(workload):\n",
    "        \n",
    "        analyst = analyst_labels[time]\n",
    "        \n",
    "        # Do one round of sparse vector technique \n",
    "        \n",
    "        # Compute noisy answer by adding Laplacian noise\n",
    "        a_t = np.random.laplace(loc=0, scale=(2*k/SVTepsilon2), size=1)[0]\n",
    "        a_t_hat = (np.dot(query, x_norm)*n ) + a_t\n",
    "\n",
    "        # Difference between noisy and maintained histogram answer\n",
    "        d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "        \n",
    "        # Lazy round: use synthetic base to answer the query\n",
    "        if (abs(d_t_hat) <= T + rho):\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            lazy_round()\n",
    "            continue\n",
    "\n",
    "        # update round: update histogram and return noisy answer\n",
    "        else:\n",
    "            #make a new noisy query answer using some of the leftover budget\n",
    "            a_t = np.random.laplace(loc=0, scale=(2*k/eps), size=1)[0]\n",
    "            a_t_hat = (np.dot(query, x_norm)*n ) + a_t\n",
    "            d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            update_times.append(time)\n",
    "            \n",
    "            # step a\n",
    "            if d_t_hat < 0:\n",
    "                r_t = query\n",
    "            else:\n",
    "                r_t = np.ones(m) - query\n",
    "            for i, v in enumerate(y_t):\n",
    "                y_t[i] = x_list[time][i] * math.exp((d_t_hat/(2*n)) * query[i]) * 20 # 20 is the learning rate\n",
    "            \n",
    "            # step b\n",
    "            x_t = y_t / np.sum(y_t)\n",
    "            update_count = update_list.count('yes')\n",
    "            \n",
    "            # if threshold for num updates is reached, just do a lazy round (synthetic database) answer\n",
    "            if update_steps[analyst] == 0: \n",
    "                if show_failure_step:\n",
    "                    print(f'Failure mode reached at query number {time}: {query}')\n",
    "                lazy_round()\n",
    "                \n",
    "            # if there are still update steps that the analyst can use, \n",
    "            # 1. update the synthetic database\n",
    "            # 2. answer the query using the noisy answer from the database itself \n",
    "            else: \n",
    "                x_list.append(x_t.round(3))\n",
    "                update_list.append('yes') # increment number of updates counter\n",
    "                pmw_answers.append(a_t_hat / np.sum(x))\n",
    "                update_steps[analyst] -= 1 # use one of analyst's update steps\n",
    "\n",
    "    update_count = update_list.count('yes')      \n",
    "\n",
    "    # calculate error\n",
    "    real_ans = np.matmul(workload, x_norm)\n",
    "    abs_error = np.abs(pmw_answers - real_ans)\n",
    "    rel_error = np.abs(abs_error / np.where(real_ans == 0, 0.000001,\n",
    "                                                real_ans))\n",
    "    \n",
    "    if show_messages:\n",
    "        np.set_printoptions(suppress=True)\n",
    "        \"\"\"Print inputes/outputs to analyze each query\"\"\"\n",
    "        print(f'Original database: {x}\\n')\n",
    "        print(f'Normalized database: {x_norm}\\n')\n",
    "        print(f'Updated Database = {x_t}\\n')\n",
    "        print(f'Update Count = {update_count}\\n')\n",
    "        print(f'{T=}\\n')\n",
    "        print(f'Error Scale Query Answer= {2*((2*k/eps)**2)}\\n')\n",
    "        print(f'Error Scale SVT= {2*((2*k/SVTepsilon2)**2)}\\n')\n",
    "        print(f'Update Parameter Scale = {eta}\\n')\n",
    "        print(f'{delta=}\\n')\n",
    "        \n",
    "    if show_plot: \n",
    "        plt.title('Error across queries:')\n",
    "        rel_line, = plt.plot(rel_error, label='Relative Error')\n",
    "        abs_line, = plt.plot(abs_error, label='Absolute Error')\n",
    "        for xc in update_times:\n",
    "            plt.axvline(x=xc, color='red', label='Update Times', linestyle='dashed')\n",
    "        plt.legend(handles=[abs_line,rel_line])\n",
    "        plt.xticks(range(0, len(workload), round(len(workload)/5)))\n",
    "    \n",
    "    if to_return == \"pd\":\n",
    "        # hacky fix: remove the first synthetic database to keep length of lists consistent with the\n",
    "        # other lists that comprise of the pandas dataframe\n",
    "        x_list.pop(0).tolist() \n",
    "        d = {\n",
    "            'algo_ans': pmw_answers,\n",
    "            'real_ans': real_ans.tolist(),\n",
    "            'queries': workload.tolist(), \n",
    "            'updated': update_list,\n",
    "            'abs_error': abs_error,               \n",
    "            'rel_error': rel_error,\n",
    "            'synthetic database': x_list,\n",
    "            'analyst': analyst_labels,\n",
    "            'd_t_hat': d_t_hat_list, \n",
    "\n",
    "             }\n",
    "        test_data = pd.DataFrame(data=d)\n",
    "        test_data = test_data.round(3)\n",
    "        return test_data\n",
    "    \n",
    "    if to_return == \"error\":\n",
    "        d = {'analyst': analyst_labels,\n",
    "             'abs_error': abs_error,               \n",
    "             'rel_error': rel_error,}\n",
    "        data = pd.DataFrame(data=d)\n",
    "        data = data.round(3)\n",
    "        \n",
    "        analyst_error = {}\n",
    "        for analyst in list(set(analyst_labels)):\n",
    "            analyst_error[analyst] = data[data.analyst==analyst]['abs_error'].sum()\n",
    "        return analyst_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea17ca88",
   "metadata": {},
   "source": [
    "### Initializing workloads and databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a073ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_small = np.array([20, 160, 20, 20, 20, 160, 20, 20])\n",
    "normalized = x_small / x_small.sum()\n",
    "m = x_small.size  # database len\n",
    "n = x_small.sum()\n",
    "#print(f'the threshold for failure is {n * math.log(m, np.e) ** (1 / 2)}')\n",
    "\n",
    "random_array = np.random.randint(2, size=(500,4))\n",
    "zero_array = np.zeros((500,4))\n",
    "alice = np.hstack((random_array, zero_array))\n",
    "bob = np.hstack((zero_array, random_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36962c66",
   "metadata": {},
   "source": [
    "## Round Robin Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced25535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alice_first</th>\n",
       "      <th>individual</th>\n",
       "      <th>bob_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1.43</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>1.38</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alice_first  individual  bob_first\n",
       "Alice         1.43        1.98       1.54\n",
       "Bob           1.38        1.92       1.47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize\n",
    "s1random_array = np.random.randint(2, size=(25,4))\n",
    "s1zero_array = np.zeros((25,4))\n",
    "s1alice_q = np.hstack((s1random_array, s1zero_array))\n",
    "s1bob_q = np.hstack((s1zero_array, s1random_array))\n",
    "\n",
    "s1combined_list = []\n",
    "s1bobfirst_list = []\n",
    "s1individual_list = []\n",
    "\n",
    "# combined\n",
    "s1combined = pmw2(workload=np.vstack((s1alice_q, s1bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                             analyst_labels=['Alice'] * 25 + ['Bob'] * 25, \n",
    "                             to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "s1combined_list.append(s1combined)\n",
    "\n",
    "s1bobfirst = pmw2(workload=np.vstack((s1bob_q, s1alice_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                             analyst_labels=['Bob'] * 25 + ['Alice'] * 25, \n",
    "                             to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "s1bobfirst_list.append(s1bobfirst)\n",
    "\n",
    "# individual\n",
    "s1a = pmw2(workload=s1alice_q, x=x_small, eps=1, T=40, k=5,\n",
    "           analyst_labels=['Alice'] * 25, \n",
    "           to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "s1b = pmw2(workload=s1bob_q, x=x_small, eps=1, T=40, k=5,\n",
    "           analyst_labels=['Bob'] * 25, \n",
    "           to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "s1individual = {**s1a, **s1b}\n",
    "s1individual_list.append(s1individual)\n",
    "    \n",
    "# find mean over multiple trials\n",
    "s1combined_average = dict(pd.DataFrame(s1combined_list).mean())\n",
    "s1bobfirst_average = dict(pd.DataFrame(s1bobfirst_list).mean())\n",
    "s1individual_average = dict(pd.DataFrame(s1individual_list).mean())\n",
    "\n",
    "d = {'alice_first': s1combined_average, 'individual': s1individual_average, 'bob_first': s1bobfirst_average}\n",
    "df = pd.DataFrame(data=d).sort_index()\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0718e77e574b71e9f7991c7da6831896cfd7281e366db0dbf84de44e8d5f66e5a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
