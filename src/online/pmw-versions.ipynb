{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fd760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import src.hdmm.workload as workload\n",
    "import src.census_workloads as census\n",
    "from src.workload_selection import workload_selection\n",
    "import online_workloads as online_workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc41a7",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf88726e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"migration_tworace.csv\"\n",
    "x_data = pd.read_csv(data_path, header=None).to_numpy().T[1]\n",
    "n = x_data.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4585091",
   "metadata": {},
   "source": [
    "# Workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d9b822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Workload: identity---\n",
      "Shape: (86, 86)\n",
      "Workload: \n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      "---Workload: total---\n",
      "Shape: (1, 86)\n",
      "Workload: \n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "---Workload: race1---\n",
      "Shape: (7, 64)\n",
      "Workload: \n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "---Workload: race2---\n",
      "Shape: (70, 64)\n",
      "Workload: \n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]]\n",
      "\n",
      "---Workload: race3---\n",
      "Shape: (1, 64)\n",
      "Workload: \n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "---Workload: custom---\n",
      "Shape: (26, 86)\n",
      "Workload: \n",
      "[[9.93178894e-01 1.73194487e+00 3.05876354e-01 ... 1.76077073e-03\n",
      "  5.08681242e-01 8.23968663e-01]\n",
      " [2.72990603e+00 5.60069032e-01 1.62837553e+00 ... 2.84375504e+00\n",
      "  2.27003780e+00 8.58984114e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00]\n",
      " ...\n",
      " [2.75729971e+00 1.14005414e+00 2.65955509e+00 ... 1.27074697e+00\n",
      "  1.40702154e+00 3.49198880e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 ... 0.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00]]\n",
      "\n",
      "---Workload: prefix_sum---\n",
      "Shape: (86, 86)\n",
      "Workload: \n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W_name = ['identity', 'total', 'race1', 'race2', 'race3', 'custom', 'prefix_sum']\n",
    "W_lst = [online_workloads.identity(n), online_workloads.total(n), online_workloads.race1(), online_workloads.race2(), online_workloads.race3(), online_workloads.custom(n), online_workloads.prefix_sum(n)]\n",
    "\n",
    "def print_workload(workload_name, workload):\n",
    "    print(f'---Workload: {workload_name}---')\n",
    "    print(f'Shape: {workload.shape}')\n",
    "    print(f'Workload: \\n{workload}\\n')\n",
    "    \n",
    "def print_workloads():\n",
    "    for i in range(7):\n",
    "        print_workload(W_name[i], W_lst[i])\n",
    "        \n",
    "print_workloads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d196db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(online_workloads.identity(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501aa469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([412, 333, 285, 231, 202, 174, 160, 142, 146, 149, 145, 181, 174,\n",
       "       190, 213, 287, 372, 499, 619, 715, 785, 821, 822, 816, 799, 742,\n",
       "       717, 697, 658, 593, 564, 519, 447, 403, 388, 365, 336, 306, 311,\n",
       "       289, 261, 231, 213, 196, 194, 170, 175, 168, 149, 142, 131, 119,\n",
       "       112, 118, 114, 116, 112, 114, 106, 111, 109, 112, 113, 109, 104,\n",
       "       108, 108,  94,  91,  81,  81,  72,  68,  63,  56,  46,  41,  38,\n",
       "        34,  28,  23,  22,  18,  18,  16,  41])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdeb9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure mode reached at query number 5: [1. 0. 0. 0. 0.]\n",
      "Failure mode reached at query number 6: [0. 1. 0. 0. 0.]\n",
      "Failure mode reached at query number 7: [0. 0. 1. 0. 0.]\n",
      "Failure mode reached at query number 8: [0. 0. 0. 1. 0.]\n",
      "Failure mode reached at query number 9: [0. 0. 0. 0. 1.]\n",
      "Original database: [1 2 3 4 5]\n",
      "\n",
      "Normalized database: [0.06666667 0.13333333 0.2        0.26666667 0.33333333]\n",
      "\n",
      "Updated Database = [0. 0. 1. 0. 0.]\n",
      "\n",
      "Update Count = 5\n",
      "\n",
      "T=5\n",
      "\n",
      "Error Scale Query Answer= 2000000.0\n",
      "\n",
      "Error Scale SVT= 32000000.0\n",
      "\n",
      "Update Parameter Scale = 0.29081910083756185\n",
      "\n",
      "delta=0.024617958204590337\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queries</th>\n",
       "      <th>synthetic database (after query)</th>\n",
       "      <th>algo_ans</th>\n",
       "      <th>real_ans</th>\n",
       "      <th>updated</th>\n",
       "      <th>abs_error</th>\n",
       "      <th>rel_error</th>\n",
       "      <th>analyst</th>\n",
       "      <th>d_t_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.007, 0.248, 0.248, 0.248, 0.248]</td>\n",
       "      <td>-6.841</td>\n",
       "      <td>0.067</td>\n",
       "      <td>yes</td>\n",
       "      <td>6.907</td>\n",
       "      <td>103.610</td>\n",
       "      <td>Alice</td>\n",
       "      <td>-105.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.009, 0.0, 0.33, 0.33, 0.33]</td>\n",
       "      <td>-66.870</td>\n",
       "      <td>0.133</td>\n",
       "      <td>yes</td>\n",
       "      <td>67.004</td>\n",
       "      <td>502.527</td>\n",
       "      <td>Alice</td>\n",
       "      <td>-1006.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>70.016</td>\n",
       "      <td>0.200</td>\n",
       "      <td>yes</td>\n",
       "      <td>69.816</td>\n",
       "      <td>349.078</td>\n",
       "      <td>Alice</td>\n",
       "      <td>1045.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>-98.530</td>\n",
       "      <td>0.267</td>\n",
       "      <td>yes</td>\n",
       "      <td>98.796</td>\n",
       "      <td>370.486</td>\n",
       "      <td>Alice</td>\n",
       "      <td>-1477.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>-14.395</td>\n",
       "      <td>0.333</td>\n",
       "      <td>yes</td>\n",
       "      <td>14.729</td>\n",
       "      <td>44.186</td>\n",
       "      <td>Alice</td>\n",
       "      <td>-215.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>no</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Alice</td>\n",
       "      <td>958.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.133</td>\n",
       "      <td>no</td>\n",
       "      <td>0.133</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Alice</td>\n",
       "      <td>372.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>no</td>\n",
       "      <td>0.800</td>\n",
       "      <td>4.000</td>\n",
       "      <td>Alice</td>\n",
       "      <td>-2022.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.267</td>\n",
       "      <td>no</td>\n",
       "      <td>0.267</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Alice</td>\n",
       "      <td>-101.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>no</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Alice</td>\n",
       "      <td>927.228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     queries     synthetic database (after query)  algo_ans  \\\n",
       "0  [1.0, 0.0, 0.0, 0.0, 0.0]  [0.007, 0.248, 0.248, 0.248, 0.248]    -6.841   \n",
       "1  [0.0, 1.0, 0.0, 0.0, 0.0]       [0.009, 0.0, 0.33, 0.33, 0.33]   -66.870   \n",
       "2  [0.0, 0.0, 1.0, 0.0, 0.0]            [0.0, 0.0, 1.0, 0.0, 0.0]    70.016   \n",
       "3  [0.0, 0.0, 0.0, 1.0, 0.0]            [0.0, 0.0, 1.0, 0.0, 0.0]   -98.530   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 1.0]            [0.0, 0.0, 1.0, 0.0, 0.0]   -14.395   \n",
       "5  [1.0, 0.0, 0.0, 0.0, 0.0]            [0.0, 0.0, 1.0, 0.0, 0.0]     0.000   \n",
       "6  [0.0, 1.0, 0.0, 0.0, 0.0]            [0.0, 0.0, 1.0, 0.0, 0.0]     0.000   \n",
       "7  [0.0, 0.0, 1.0, 0.0, 0.0]            [0.0, 0.0, 1.0, 0.0, 0.0]     1.000   \n",
       "8  [0.0, 0.0, 0.0, 1.0, 0.0]            [0.0, 0.0, 1.0, 0.0, 0.0]     0.000   \n",
       "9  [0.0, 0.0, 0.0, 0.0, 1.0]            [0.0, 0.0, 1.0, 0.0, 0.0]     0.000   \n",
       "\n",
       "   real_ans updated  abs_error  rel_error analyst   d_t_hat  \n",
       "0     0.067     yes      6.907    103.610   Alice  -105.610  \n",
       "1     0.133     yes     67.004    502.527   Alice -1006.774  \n",
       "2     0.200     yes     69.816    349.078   Alice  1045.283  \n",
       "3     0.267     yes     98.796    370.486   Alice -1477.944  \n",
       "4     0.333     yes     14.729     44.186   Alice  -215.929  \n",
       "5     0.067      no      0.067      1.000   Alice   958.767  \n",
       "6     0.133      no      0.133      1.000   Alice   372.748  \n",
       "7     0.200      no      0.800      4.000   Alice -2022.990  \n",
       "8     0.267      no      0.267      1.000   Alice  -101.752  \n",
       "9     0.333      no      0.333      1.000   Alice   927.228  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo: Add code to make epsilon adjustable proportionally. \n",
    "# Todo: Fix divide by zero error (<ipython-input-51-46063bd03d8a>:37: RuntimeWarning: divide by zero encountered in double_scalars delta = 1 / (n * math.log(n, np.e)))\n",
    "\n",
    "def pmw_split(workload, x, analyst_labels, T=5, eps=0.01, k=5, show_messages=True, to_return='pd', show_plot=False, show_failure_step=True):\n",
    "    \"\"\"\n",
    "    Implement Private Multiplicative Weights Mechanism (PMW) on a workload of\n",
    "    linear queries. \n",
    "\n",
    "    Algorithm Parameters: \n",
    "    - workload = workload of queries (M x k numpy array)\n",
    "    - x = true database (M x 1 numpy array)\n",
    "    - T = update threshold\n",
    "    - eps = privacy budget\n",
    "    - k = number of update steps PER ANALYST\n",
    "    - analyst_labels = list of analyst names corresponding to each query in the workload\n",
    "    \n",
    "    Output Controls: \n",
    "    - show_messages argument determines whether the function will print information such as \n",
    "    error scale, threshold, update steps used, etc.\n",
    "    - to_return argument determines what the function will return. \n",
    "        - if 'pd', pmw() returns pandas df with test data for each \n",
    "        query in the workload(showing query, d_t_hat, updated, algo_ans, real_ans, \n",
    "        abs_error, rel_error). \n",
    "        - if 'update_count', pmw() returns the update count for the total\n",
    "        amount of queries.\n",
    "    - show_plot - T/F whether the function will display a plot\n",
    "    - show_failure_step - T/F whether function prints what step failure mode is reached\n",
    "    \"\"\" \n",
    "    \n",
    "    update_steps = {}\n",
    "    for analyst in list(set(analyst_labels)): \n",
    "        update_steps[analyst] = k # each analyst starts with k update steps\n",
    "    \n",
    "    # initialize constants\n",
    "    m = x.size  # database len\n",
    "    n = x.sum()\n",
    "    eta = (math.log(m, np.e) ** (1 / 4)) / (math.sqrt(n))\n",
    "    delta = 1 / (n * math.log(n, np.e))\n",
    "    x_norm = x / np.sum(x)\n",
    "    \n",
    "    # initialize synthetic databases at time 0 (prior to any queries)\n",
    "    x_t = np.ones(m) / m\n",
    "    y_t = np.ones(m) / m\n",
    "\n",
    "    # initialize tracker lists to construct pandas dataframe at the end \n",
    "    x_list = [x_t] # create a list of x_t synthetic database at every time step\n",
    "    update_list = []\n",
    "    update_count = 0\n",
    "    pmw_answers = []\n",
    "    update_times = [] # record times that database is updated\n",
    "    d_t_hat_list = []\n",
    "    \n",
    "    def lazy_round():\n",
    "        \"\"\"\n",
    "        \"Lazy Round\" of querying using the stored synthetic database, x_t, in list x_list.\n",
    "        p\n",
    "        We call this the lazy round because it is contrasted with the updated step where we update the \n",
    "        sythetic database and answer the query using the real database.\n",
    "        \"\"\"\n",
    "        update_list.append('no')\n",
    "        pmw_answers.append(np.dot(query, x_list[time]))\n",
    "        x_list.append(x_list[time].round(3))\n",
    "    \n",
    "    # inititate first instance of SVT with half the budget and k updates; will be reset in the main loop\n",
    "    SVTtrigger = False \n",
    "    SVTepsilon1 = ((eps/2)/2)\n",
    "    SVTepsilon2 = ((eps/2)/2)\n",
    "    rho = np.random.laplace(loc=0, scale=(1/SVTepsilon1), size=1)[0]\n",
    "    \n",
    "    for time, query in enumerate(workload):\n",
    "        \n",
    "        analyst = analyst_labels[time]\n",
    "        \n",
    "        # Do one round of sparse vector technique \n",
    "        \n",
    "        # Compute noisy answer by adding Laplacian noise\n",
    "        a_t = np.random.laplace(loc=0, scale=(2*k/SVTepsilon2), size=1)[0]\n",
    "    \n",
    "        a_t_hat = (np.dot(query, x_norm)*n ) + a_t\n",
    "        \n",
    "        # Difference between noisy and maintained histogram answer\n",
    "        d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "        \n",
    "        # Lazy round: use synthetic base to answer the query\n",
    "        if (abs(d_t_hat) <= T + rho):\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            lazy_round()\n",
    "            continue\n",
    "\n",
    "        # update round: update histogram and return noisy answer\n",
    "        else:\n",
    "            #make a new noisy query answer using some of the leftover budget\n",
    "            a_t = np.random.laplace(loc=0, scale=(2*k/eps), size=1)[0]\n",
    "            a_t_hat = (np.dot(query, x_norm)*n ) + a_t\n",
    "            d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            update_times.append(time)\n",
    "            \n",
    "            # step a\n",
    "            if d_t_hat < 0:\n",
    "                r_t = query\n",
    "            else:\n",
    "                r_t = np.ones(m) - query\n",
    "            for i, v in enumerate(y_t):\n",
    "                y_t[i] = x_list[time][i] * math.exp((d_t_hat/(2*n)) * query[i]) * 20 # 20 is the learning rate\n",
    "            \n",
    "            # step b\n",
    "            x_t = y_t / np.sum(y_t)\n",
    "            update_count = update_list.count('yes')\n",
    "            \n",
    "            # if threshold for num updates is reached, just do a lazy round (synthetic database) answer\n",
    "            if update_steps[analyst] == 0: \n",
    "                if show_failure_step:\n",
    "                    print(f'Failure mode reached at query number {time}: {query}')\n",
    "                lazy_round()\n",
    "                \n",
    "            # if there are still update steps that the analyst can use, \n",
    "            # 1. update the synthetic database\n",
    "            # 2. answer the query using the noisy answer from the database itself \n",
    "            else: \n",
    "                x_list.append(x_t.round(3))\n",
    "                update_list.append('yes') # increment number of updates counter\n",
    "                pmw_answers.append(a_t_hat / np.sum(x))\n",
    "                update_steps[analyst] -= 1 # use one of analyst's update steps\n",
    "\n",
    "    update_count = update_list.count('yes')      \n",
    "\n",
    "    # calculate error\n",
    "    real_ans = np.matmul(workload, x_norm)\n",
    "    abs_error = np.abs(pmw_answers - real_ans)\n",
    "    rel_error = np.abs(abs_error / np.where(real_ans == 0, 0.000001,\n",
    "                                                real_ans))\n",
    "    \n",
    "    if show_messages:\n",
    "        np.set_printoptions(suppress=True)\n",
    "        \"\"\"Print inputes/outputs to analyze each query\"\"\"\n",
    "        print(f'Original database: {x}\\n')\n",
    "        print(f'Normalized database: {x_norm}\\n')\n",
    "        print(f'Updated Database = {x_t}\\n')\n",
    "        print(f'Update Count = {update_count}\\n')\n",
    "        print(f'{T=}\\n')\n",
    "        print(f'Error Scale Query Answer= {2*((2*k/eps)**2)}\\n')\n",
    "        print(f'Error Scale SVT= {2*((2*k/SVTepsilon2)**2)}\\n')\n",
    "        print(f'Update Parameter Scale = {eta}\\n')\n",
    "        print(f'{delta=}\\n')\n",
    "        \n",
    "    if show_plot: \n",
    "        plt.title('Error across queries:')\n",
    "        rel_line, = plt.plot(rel_error, label='Relative Error')\n",
    "        abs_line, = plt.plot(abs_error, label='Absolute Error')\n",
    "        for xc in update_times:\n",
    "            plt.axvline(x=xc, color='red', label='Update Times', linestyle='dashed')\n",
    "        plt.legend(handles=[abs_line,rel_line])\n",
    "        plt.xticks(range(0, len(workload), round(len(workload)/5)))\n",
    "    \n",
    "    if to_return == \"pd\":\n",
    "        # hacky fix: remove the first synthetic database to keep length of lists consistent with the\n",
    "        # other lists that comprise of the pandas dataframe\n",
    "        x_list.pop(0).tolist() \n",
    "        d = {\n",
    "            'queries': workload.tolist(), \n",
    "            'synthetic database (after query)': x_list,\n",
    "            'algo_ans': pmw_answers,\n",
    "            'real_ans': real_ans.tolist(),\n",
    "            'updated': update_list,\n",
    "            'abs_error': abs_error,               \n",
    "            'rel_error': rel_error,\n",
    "            'analyst': analyst_labels,\n",
    "            'd_t_hat': d_t_hat_list, \n",
    "\n",
    "             }\n",
    "        test_data = pd.DataFrame(data=d)\n",
    "        test_data = test_data.round(3)\n",
    "        return test_data\n",
    "    \n",
    "    # return dictionary of absolute errors\n",
    "    if to_return == \"error\":\n",
    "        d = {'analyst': analyst_labels,\n",
    "             'abs_error': abs_error,               \n",
    "             'rel_error': rel_error,}\n",
    "        data = pd.DataFrame(data=d)\n",
    "        data = data.round(3)\n",
    "        \n",
    "        analyst_error = {}\n",
    "        for analyst in list(set(analyst_labels)):\n",
    "            analyst_error[analyst] = data[data.analyst==analyst]['abs_error'].sum()\n",
    "        return analyst_error\n",
    "    \n",
    "x_example = np.array([1, 2, 3, 4, 5])\n",
    "pmw_split(np.vstack((online_workloads.identity(5), online_workloads.identity(5))), x_example, ['Alice'] * 10, eps=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e1eda",
   "metadata": {},
   "source": [
    "It seems to update too much during the update steps (step 0-5). Using the example database of [1, 2, 3, 4, 5] and a workload of two stacked 5x5 identity matrices, during the first 5 queries consisting of the first identity matrix, the database updates the first few vectors way too much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2714c887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 5\n",
    "a == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f3815267",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pmw_split\n",
    "def pmw2(workload, x, analyst_labels, T, eps=0.01, k=None, \n",
    "         show_messages=False, to_return='pd', show_plot=False, show_failure_step=False):\n",
    "    \"\"\"\n",
    "    Implement Private Multiplicative Weights Mechanism (PMW) on a workload of\n",
    "    linear queries. \n",
    "\n",
    "    Algorithm Parameters: \n",
    "    - workload = workload of queries (M x k numpy array)\n",
    "    - x = true database (M x 1 numpy array)\n",
    "    - T = update threshold\n",
    "    - eps = privacy budget\n",
    "    - k = number of update steps PER ANALYST\n",
    "    - analyst_labels = list of analyst names corresponding to each query in the workload\n",
    "    \n",
    "    Output Controls: \n",
    "    - show_messages argument determines whether the function will print information such as \n",
    "    error scale, threshold, update steps used, etc.\n",
    "    - to_return argument determines what the function will return. \n",
    "        - if 'pd', pmw() returns pandas df with test data for each \n",
    "        query in the workload(showing query, d_t_hat, updated, algo_ans, real_ans, \n",
    "        abs_error, rel_error). \n",
    "        - if 'update_count', pmw() returns the update count for the total\n",
    "        amount of queries.\n",
    "    - show_plot - T/F whether the function will display a plot\n",
    "    - show_failure_step - T/F whether function prints what step failure mode is reached\n",
    "    \"\"\" \n",
    "    \n",
    "    update_steps = {}\n",
    "    for analyst in list(set(analyst_labels)): \n",
    "        update_steps[analyst] = k # each analyst starts with k update steps\n",
    "    \n",
    "    # initialize constants\n",
    "    m = x.size  # database len\n",
    "    n = x.sum()\n",
    "    eta = (math.log(m, np.e) ** (1 / 4)) / (math.sqrt(n))\n",
    "    delta = 1 / (n * math.log(n, np.e))\n",
    "    x_norm = x / np.sum(x)\n",
    "    \n",
    "    # initialize synthetic databases at time 0 (prior to any queries)\n",
    "    x_t = np.ones(m) / m\n",
    "    y_t = np.ones(m) / m\n",
    "\n",
    "    # initialize tracker lists to construct pandas dataframe at the end \n",
    "    x_list = [x_t] # create a list of x_t synthetic database at every time step\n",
    "    update_list = []\n",
    "    update_count = 0\n",
    "    pmw_answers = []\n",
    "    update_times = [] # record times that database is updated\n",
    "    d_t_hat_list = []\n",
    "    \n",
    "    def lazy_round():\n",
    "        \"\"\"\n",
    "        \"Lazy Round\" of querying using the stored synthetic database, x_t, in list x_list.\n",
    "        \n",
    "        We call this the lazy round because it is contrasted with the updated step where we update the \n",
    "        sythetic database and answer the query using the real database.\n",
    "        \"\"\"\n",
    "        update_list.append('no')\n",
    "        answer = np.dot(query, x_list[time])\n",
    "        if answer < 0:\n",
    "            pmw_answers.append(0)\n",
    "        else: \n",
    "            pmw_answers.append(answer)\n",
    "        x_list.append(x_list[time].round(3))\n",
    "    \n",
    "    # inititate first instance of SVT with half the budget and k updates; will be reset in the main loop\n",
    "    SVTtrigger = False \n",
    "    SVTepsilon1 = ((eps/2)/2)\n",
    "    SVTepsilon2 = ((eps/2)/2)\n",
    "    rho = np.random.laplace(loc=0, scale=(1/SVTepsilon1), size=1)[0]\n",
    "    \n",
    "    for time, query in enumerate(workload):\n",
    "        \n",
    "        analyst = analyst_labels[time]\n",
    "        \n",
    "        # Do one round of sparse vector technique; compute noisy answer by adding Laplacian noise\n",
    "        A_t = np.random.laplace(loc=0, scale=(k/SVTepsilon2), size=1)[0]\n",
    "        a_t_hat = (np.dot(query, x_norm)*n ) + A_t\n",
    "        d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "        \n",
    "        # LAZY ROUND: QUERY USING THE SYNTHETIC DATABASE\n",
    "        if (abs(d_t_hat) <= T + rho):\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            lazy_round()\n",
    "\n",
    "        # UPDATE ROUND: UPDATE SYNTHETIC DATABASE AND RETURN NOISY ANSWER, A_T-HAT\n",
    "        else:\n",
    "            # noise\n",
    "            A_t = np.random.laplace(loc=0, scale=(2*k/eps), size=1)[0]\n",
    "            \n",
    "            # noisy answer\n",
    "            a_t_hat = (np.dot(query, x_norm)*n ) + A_t\n",
    "            d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            update_times.append(time)\n",
    "            \n",
    "            # step a\n",
    "            if d_t_hat < 0:\n",
    "                r_t = query\n",
    "            else:\n",
    "                r_t = np.ones(m) - query\n",
    "            for i in range(len(y_t)):\n",
    "                y_t[i] = x_list[time][i] * math.exp((d_t_hat/(2*n)) * query[i]) * 20 # 20 is the learning rate\n",
    "            \n",
    "            # step b\n",
    "            x_t = y_t / np.sum(y_t)\n",
    "            update_count = update_list.count('yes')\n",
    "            \n",
    "            # if threshold for num updates is reached, just do a lazy round (synthetic database) answer\n",
    "            if update_steps[analyst] == 0: \n",
    "                if show_failure_step:\n",
    "                    print(f'Failure mode reached at query number {time}: {query}')\n",
    "                lazy_round()\n",
    "                \n",
    "            # if there are still update steps that the analyst can use, \n",
    "            # 1. update the synthetic database\n",
    "            # 2. answer the query using the noisy answer from the database itself \n",
    "            else: \n",
    "                x_list.append(x_t.round(3))\n",
    "                update_list.append('yes') # increment number of updates counter\n",
    "                answer = a_t_hat / np.sum(x)\n",
    "                \n",
    "                if answer < 0:\n",
    "                    pmw_answers.append(0)\n",
    "                else: \n",
    "                    pmw_answers.append(answer)\n",
    "                \n",
    "                update_steps[analyst] -= 1 # use one of analyst's update steps\n",
    "\n",
    "    update_count = update_list.count('yes')      \n",
    "\n",
    "    # calculate error\n",
    "    real_ans = np.matmul(workload, x_norm)\n",
    "    abs_error = np.abs(pmw_answers - real_ans)\n",
    "    rel_error = np.abs(abs_error / np.where(real_ans == 0, 0.000001,\n",
    "                                                real_ans))\n",
    "    \n",
    "    if show_messages:\n",
    "        np.set_printoptions(suppress=True)\n",
    "        \"\"\"Print inputes/outputs to analyze each query\"\"\"\n",
    "        print(f'Original database: {x}\\n')\n",
    "        print(f'Normalized database: {x_norm}\\n')\n",
    "        print(f'Updated Database = {x_t}\\n')\n",
    "        print(f'Update Count = {update_count}\\n')\n",
    "        print(f'{T=}\\n')\n",
    "        print(f'Error Scale Query Answer= {2*((2*k/eps)**2)}\\n')\n",
    "        print(f'Error Scale SVT= {2*((2*k/SVTepsilon2)**2)}\\n')\n",
    "        print(f'Update Parameter Scale = {eta}\\n')\n",
    "        print(f'{delta=}\\n')\n",
    "        \n",
    "    if show_plot: \n",
    "        plt.title('Error across queries:')\n",
    "        rel_line, = plt.plot(rel_error, label='Relative Error')\n",
    "        abs_line, = plt.plot(abs_error, label='Absolute Error')\n",
    "        for xc in update_times:\n",
    "            plt.axvline(x=xc, color='red', label='Update Times', linestyle='dashed')\n",
    "        plt.legend(handles=[abs_line,rel_line])\n",
    "        plt.xticks(range(0, len(workload), round(len(workload)/5)))\n",
    "    \n",
    "    if to_return == \"pd\":\n",
    "        # hacky fix: remove the first synthetic database to keep length of lists consistent with the\n",
    "        # other lists that comprise of the pandas dataframe\n",
    "        x_list.pop(0).tolist() \n",
    "        d = {\n",
    "            'algo_ans': pmw_answers,\n",
    "            'real_ans': real_ans.tolist(),\n",
    "            'queries': workload.tolist(), \n",
    "            'updated': update_list,\n",
    "            'abs_error': abs_error,               \n",
    "            'rel_error': rel_error,\n",
    "            'synthetic database': x_list,\n",
    "            'analyst': analyst_labels,\n",
    "            'd_t_hat': d_t_hat_list, \n",
    "\n",
    "             }\n",
    "        test_data = pd.DataFrame(data=d)\n",
    "        test_data = test_data.round(3)\n",
    "        return test_data\n",
    "    \n",
    "    if to_return == \"error\":\n",
    "        d = {'analyst': analyst_labels,\n",
    "             'abs_error': abs_error,               \n",
    "             'rel_error': rel_error,}\n",
    "        data = pd.DataFrame(data=d)\n",
    "        data = data.round(3)\n",
    "        \n",
    "        analyst_error = {}\n",
    "        for analyst in list(sorted(analyst_labels)):\n",
    "            analyst_error[analyst] = data[data.analyst==analyst]['abs_error'].sum()\n",
    "        return analyst_error\n",
    "    \n",
    "x_example = np.array([1000, 2000, 3000, 4000, 5000])\n",
    "\n",
    "pmw2(np.vstack((online_workloads.identity(5), online_workloads.identity(5))), \n",
    "     x_example, ['A'] * 5 + ['B'] * 5, eps=1, T=40, k = 5, to_return='error_vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4575bffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': array([[1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.]]), 'B': array([[0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A': 0.344, 'B': 0.371}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pmw_independent: write pmw for one person. \n",
    "# create wrapper function called pmw_independent() that takes in the workloads and workload labels. Run PMW for each analyst, separate their workloads based on analysts. \n",
    "\n",
    "def pmw_independent(w, input_x, analyst_labels, input_T, input_eps=0.01, input_k=5):\n",
    "    \"\"\"\n",
    "    Wrapper function that calls pmw2() to simulate PMW for each independent person. \n",
    "    \n",
    "    Takes a stream of workloads and analyst labels and separates them into distinct workloads for each analyst. Runs\n",
    "    pmw2() on that particular workload for each analyst. Returns a dictionary o \n",
    "    \"\"\"\n",
    "    indices = {} # k: analyst, v: row indices of queries in the workloads\n",
    "    for i, analyst in enumerate(analyst_labels):\n",
    "        if analyst not in indices.keys(): \n",
    "            indices[analyst] = []\n",
    "        indices[analyst].append(i)\n",
    "\n",
    "    workloads = {} # k: analyst, v: the analyst's workload\n",
    "    for analyst in indices.keys():\n",
    "        workloads[analyst] = w[indices[analyst], :]\n",
    "    print(workloads)\n",
    "\n",
    "    all_analyst_error_dic = {}\n",
    "    \n",
    "    for analyst in workloads.keys():\n",
    "        single_analyst_error = pmw2(workload=workloads[analyst], \n",
    "                                    x=input_x, \n",
    "                                    T=input_T, \n",
    "                                    k = input_k,\n",
    "                                    analyst_labels=[analyst]*len(workloads[analyst]), \n",
    "                                    to_return=\"error\",\n",
    "                                    show_messages=False)\n",
    "        all_analyst_error_dic.update(single_analyst_error)\n",
    "    return all_analyst_error_dic\n",
    "             \n",
    "        \n",
    "pmw_independent(np.vstack((online_workloads.identity(5), \n",
    "                           online_workloads.identity(5))), \n",
    "                input_x=x_example, \n",
    "                input_T=40, \n",
    "                input_eps=1, \n",
    "                analyst_labels=['A'] * 2 + ['B'] * 6 + ['A'] * 2, \n",
    "                )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "98a19abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_k=12071\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo_ans</th>\n",
       "      <th>real_ans</th>\n",
       "      <th>queries</th>\n",
       "      <th>updated</th>\n",
       "      <th>abs_error</th>\n",
       "      <th>rel_error</th>\n",
       "      <th>synthetic database</th>\n",
       "      <th>analyst</th>\n",
       "      <th>d_t_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.000</td>\n",
       "      <td>[0.082, 0.229, 0.229, 0.229, 0.229]</td>\n",
       "      <td>A</td>\n",
       "      <td>-30789.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.162</td>\n",
       "      <td>0.133</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.217</td>\n",
       "      <td>[0.083, 0.224, 0.231, 0.231, 0.231]</td>\n",
       "      <td>A</td>\n",
       "      <td>-1000.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.000</td>\n",
       "      <td>[0.098, 0.265, 0.09, 0.273, 0.273]</td>\n",
       "      <td>A</td>\n",
       "      <td>-33444.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.911</td>\n",
       "      <td>0.267</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>4.644</td>\n",
       "      <td>17.416</td>\n",
       "      <td>[0.028, 0.076, 0.026, 0.793, 0.078]</td>\n",
       "      <td>A</td>\n",
       "      <td>69569.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.780</td>\n",
       "      <td>0.333</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1.340</td>\n",
       "      <td>[0.027, 0.074, 0.025, 0.767, 0.107]</td>\n",
       "      <td>A</td>\n",
       "      <td>10528.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.057</td>\n",
       "      <td>0.067</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.152</td>\n",
       "      <td>[0.027, 0.074, 0.025, 0.767, 0.107]</td>\n",
       "      <td>B</td>\n",
       "      <td>442.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.735</td>\n",
       "      <td>0.133</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.601</td>\n",
       "      <td>4.510</td>\n",
       "      <td>[0.026, 0.1, 0.024, 0.745, 0.104]</td>\n",
       "      <td>B</td>\n",
       "      <td>9910.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.697</td>\n",
       "      <td>0.200</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.497</td>\n",
       "      <td>2.485</td>\n",
       "      <td>[0.026, 0.099, 0.033, 0.739, 0.103]</td>\n",
       "      <td>B</td>\n",
       "      <td>10095.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.109</td>\n",
       "      <td>0.267</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.592</td>\n",
       "      <td>[0.032, 0.124, 0.041, 0.674, 0.129]</td>\n",
       "      <td>B</td>\n",
       "      <td>-9451.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>[0.034, 0.131, 0.043, 0.712, 0.08]</td>\n",
       "      <td>B</td>\n",
       "      <td>-16075.884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   algo_ans  real_ans                    queries updated  abs_error  \\\n",
       "0     0.000     0.067  [1.0, 0.0, 0.0, 0.0, 0.0]     yes      0.067   \n",
       "1     0.162     0.133  [0.0, 1.0, 0.0, 0.0, 0.0]     yes      0.029   \n",
       "2     0.000     0.200  [0.0, 0.0, 1.0, 0.0, 0.0]     yes      0.200   \n",
       "3     4.911     0.267  [0.0, 0.0, 0.0, 1.0, 0.0]     yes      4.644   \n",
       "4     0.780     0.333  [0.0, 0.0, 0.0, 0.0, 1.0]     yes      0.447   \n",
       "5     0.057     0.067  [1.0, 0.0, 0.0, 0.0, 0.0]     yes      0.010   \n",
       "6     0.735     0.133  [0.0, 1.0, 0.0, 0.0, 0.0]     yes      0.601   \n",
       "7     0.697     0.200  [0.0, 0.0, 1.0, 0.0, 0.0]     yes      0.497   \n",
       "8     0.109     0.267  [0.0, 0.0, 0.0, 1.0, 0.0]     yes      0.158   \n",
       "9     0.000     0.333  [0.0, 0.0, 0.0, 0.0, 1.0]     yes      0.333   \n",
       "\n",
       "   rel_error                   synthetic database analyst    d_t_hat  \n",
       "0      1.000  [0.082, 0.229, 0.229, 0.229, 0.229]       A -30789.864  \n",
       "1      0.217  [0.083, 0.224, 0.231, 0.231, 0.231]       A  -1000.901  \n",
       "2      1.000   [0.098, 0.265, 0.09, 0.273, 0.273]       A -33444.208  \n",
       "3     17.416  [0.028, 0.076, 0.026, 0.793, 0.078]       A  69569.955  \n",
       "4      1.340  [0.027, 0.074, 0.025, 0.767, 0.107]       A  10528.852  \n",
       "5      0.152  [0.027, 0.074, 0.025, 0.767, 0.107]       B    442.580  \n",
       "6      4.510    [0.026, 0.1, 0.024, 0.745, 0.104]       B   9910.453  \n",
       "7      2.485  [0.026, 0.099, 0.033, 0.739, 0.103]       B  10095.999  \n",
       "8      0.592  [0.032, 0.124, 0.041, 0.674, 0.129]       B  -9451.760  \n",
       "9      1.000   [0.034, 0.131, 0.043, 0.712, 0.08]       B -16075.884  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pmw_naive\n",
    "# write version of PMW where analysts can run out of privacy budget if they use too much of others' budgets\n",
    "# this version everyone shares the same number of update steps \n",
    "def pmw_naive(workload, x, analyst_labels, T, eps=0.01, total_k=None, \n",
    "         show_messages=False, to_return='pd', show_plot=False, show_failure_step=False):\n",
    "    \"\"\"\n",
    "    Implement Private Multiplicative Weights Mechanism (PMW) on a workload of\n",
    "    linear queries where analysts can run out of privacy budget if they use too much of others'. \n",
    "    \n",
    "    In other words, \n",
    "\n",
    "    Algorithm Parameters: \n",
    "    - workload = workload of queries (M x k numpy array)\n",
    "    - x = true database (M x 1 numpy array)\n",
    "    - T = update threshold\n",
    "    - eps = privacy budget\n",
    "    - total_k = total number of update steps alloted for the entire group\n",
    "    - analyst_labels = list of analyst names corresponding to each query in the workload\n",
    "    \n",
    "    Output Controls: \n",
    "    - show_messages argument determines whether the function will print information such as \n",
    "    error scale, threshold, update steps used, etc.\n",
    "    - to_return argument determines what the function will return. \n",
    "        - if 'pd', pmw() returns pandas df with test data for each \n",
    "        query in the workload(showing query, d_t_hat, updated, algo_ans, real_ans, \n",
    "        abs_error, rel_error). \n",
    "        - if 'update_count', pmw() returns the update count for the total\n",
    "        amount of queries.\n",
    "    - show_plot - T/F whether the function will display a plot\n",
    "    - show_failure_step - T/F whether function prints what step failure mode is reached\n",
    "    \"\"\" \n",
    "    \n",
    "    # initialize constants\n",
    "    m = x.size  # database len\n",
    "    n = x.sum()\n",
    "    eta = (math.log(m, np.e) ** (1 / 4)) / (math.sqrt(n))\n",
    "    delta = 1 / (n * math.log(n, np.e))\n",
    "    x_norm = x / np.sum(x)\n",
    "    \n",
    "    # initialize synthetic databases at time 0 (prior to any queries)\n",
    "    x_t = np.ones(m) / m\n",
    "    y_t = np.ones(m) / m\n",
    "\n",
    "    # initialize tracker lists to construct pandas dataframe at the end \n",
    "    x_list = [x_t] # create a list of x_t synthetic database at every time step\n",
    "    update_list = []\n",
    "    update_count = 0\n",
    "    pmw_answers = []\n",
    "    update_times = [] # record times that database is updated\n",
    "    d_t_hat_list = []\n",
    "    \n",
    "    # initialize total_k, the total number of update steps if not default\n",
    "    if total_k == None:\n",
    "        total_k = round(n * math.log(math.sqrt(m)))\n",
    "        print(f'{total_k=}')\n",
    "    \n",
    "    def lazy_round():\n",
    "        \"\"\"\n",
    "        \"Lazy Round\" of querying using the stored synthetic database, x_t, in list x_list.\n",
    "        \n",
    "        We call this the lazy round because it is contrasted with the updated step where we update the \n",
    "        sythetic database and answer the query using the real database.\n",
    "        \"\"\"\n",
    "        update_list.append('no')\n",
    "        answer = np.dot(query, x_list[time])\n",
    "        if answer < 0:\n",
    "            pmw_answers.append(0)\n",
    "        else: \n",
    "            pmw_answers.append(answer)\n",
    "        x_list.append(x_list[time].round(3))\n",
    "    \n",
    "    # inititate first instance of SVT with half the budget and k updates; will be reset in the main loop\n",
    "    SVTtrigger = False \n",
    "    SVTepsilon1 = ((eps/2)/2)\n",
    "    SVTepsilon2 = ((eps/2)/2)\n",
    "    rho = np.random.laplace(loc=0, scale=(1/SVTepsilon1), size=1)[0]\n",
    "    \n",
    "    for time, query in enumerate(workload):\n",
    "        \n",
    "        analyst = analyst_labels[time]\n",
    "        \n",
    "        # Do one round of sparse vector technique; compute noisy answer by adding Laplacian noise\n",
    "        A_t = np.random.laplace(loc=0, scale=(total_k/SVTepsilon2), size=1)[0]\n",
    "        a_t_hat = (np.dot(query, x_norm)*n ) + A_t\n",
    "        d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "        \n",
    "        # LAZY ROUND: QUERY USING THE SYNTHETIC DATABASE\n",
    "        if (abs(d_t_hat) <= T + rho):\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            lazy_round()\n",
    "\n",
    "        # UPDATE ROUND: UPDATE SYNTHETIC DATABASE AND RETURN NOISY ANSWER, A_T-HAT\n",
    "        else:\n",
    "            # noise\n",
    "            A_t = np.random.laplace(loc=0, scale=(2*total_k/eps), size=1)[0]\n",
    "            \n",
    "            # noisy answer\n",
    "            a_t_hat = (np.dot(query, x_norm)*n ) + A_t\n",
    "            d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            update_times.append(time)\n",
    "            \n",
    "            # step a\n",
    "            if d_t_hat < 0:\n",
    "                r_t = query\n",
    "            else:\n",
    "                r_t = np.ones(m) - query\n",
    "            for i in range(len(y_t)):\n",
    "                y_t[i] = x_list[time][i] * math.exp((d_t_hat/(2*n)) * query[i]) * 20 # 20 is the learning rate\n",
    "            \n",
    "            # step b\n",
    "            x_t = y_t / np.sum(y_t)\n",
    "            update_count = update_list.count('yes')\n",
    "            \n",
    "            # if threshold for num updates is reached, just do a lazy round (synthetic database) answer\n",
    "            if total_k == 0: \n",
    "                if show_failure_step:\n",
    "                    print(f'Failure mode reached at query number {time}: {query}')\n",
    "                lazy_round()\n",
    "                \n",
    "            # if there are still update steps that the analyst can use, \n",
    "            # 1. update the synthetic database\n",
    "            # 2. answer the query using the noisy answer from the database itself \n",
    "            else: \n",
    "                x_list.append(x_t.round(3))\n",
    "                update_list.append('yes') # increment number of updates counter\n",
    "                answer = a_t_hat / np.sum(x)\n",
    "                \n",
    "                if answer < 0:\n",
    "                    pmw_answers.append(0)\n",
    "                else: \n",
    "                    pmw_answers.append(answer)\n",
    "                \n",
    "                total_k -= 1 # use one of the total update steps\n",
    "\n",
    "    update_count = update_list.count('yes')      \n",
    "\n",
    "    # calculate error\n",
    "    real_ans = np.matmul(workload, x_norm)\n",
    "    abs_error = np.abs(pmw_answers - real_ans)\n",
    "    rel_error = np.abs(abs_error / np.where(real_ans == 0, 0.000001,\n",
    "                                                real_ans))\n",
    "    \n",
    "    if show_messages:\n",
    "        np.set_printoptions(suppress=True)\n",
    "        \"\"\"Print inputes/outputs to analyze each query\"\"\"\n",
    "        print(f'Original database: {x}\\n')\n",
    "        print(f'Normalized database: {x_norm}\\n')\n",
    "        print(f'Updated Database = {x_t}\\n')\n",
    "        print(f'Update Count = {update_count}\\n')\n",
    "        print(f'{T=}\\n')\n",
    "        print(f'Error Scale Query Answer= {2*((2*k/eps)**2)}\\n')\n",
    "        print(f'Error Scale SVT= {2*((2*k/SVTepsilon2)**2)}\\n')\n",
    "        print(f'Update Parameter Scale = {eta}\\n')\n",
    "        print(f'{delta=}\\n')\n",
    "        \n",
    "    if show_plot: \n",
    "        plt.title('Error across queries:')\n",
    "        rel_line, = plt.plot(rel_error, label='Relative Error')\n",
    "        abs_line, = plt.plot(abs_error, label='Absolute Error')\n",
    "        for xc in update_times:\n",
    "            plt.axvline(x=xc, color='red', label='Update Times', linestyle='dashed')\n",
    "        plt.legend(handles=[abs_line,rel_line])\n",
    "        plt.xticks(range(0, len(workload), round(len(workload)/5)))\n",
    "    \n",
    "    if to_return == \"pd\":\n",
    "        # hacky fix: remove the first synthetic database to keep length of lists consistent with the\n",
    "        # other lists that comprise of the pandas dataframe\n",
    "        x_list.pop(0).tolist() \n",
    "        d = {\n",
    "            'algo_ans': pmw_answers,\n",
    "            'real_ans': real_ans.tolist(),\n",
    "            'queries': workload.tolist(), \n",
    "            'updated': update_list,\n",
    "            'abs_error': abs_error,               \n",
    "            'rel_error': rel_error,\n",
    "            'synthetic database': x_list,\n",
    "            'analyst': analyst_labels,\n",
    "            'd_t_hat': d_t_hat_list, \n",
    "\n",
    "             }\n",
    "        test_data = pd.DataFrame(data=d)\n",
    "        test_data = test_data.round(3)\n",
    "        return test_data\n",
    "    \n",
    "    if to_return == \"error\":\n",
    "        d = {'analyst': analyst_labels,\n",
    "             'abs_error': abs_error,               \n",
    "             'rel_error': rel_error,}\n",
    "        data = pd.DataFrame(data=d)\n",
    "        data = data.round(3)\n",
    "        \n",
    "        analyst_error = {}\n",
    "        for analyst in list(sorted(analyst_labels)):\n",
    "            analyst_error[analyst] = data[data.analyst==analyst]['abs_error'].sum()\n",
    "        return analyst_error\n",
    "    \n",
    "x_example = np.array([1000, 2000, 3000, 4000, 5000])\n",
    "\n",
    "pmw_naive(np.vstack((online_workloads.identity(5), online_workloads.identity(5))), \n",
    "     x_example, ['A'] * 5 + ['B'] * 5, eps=1, T=40, total_k = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc5cb7",
   "metadata": {},
   "source": [
    "PMW independent\n",
    "\n",
    "Initialize an instance of PMW for each analyst with alpha =T and their share of the privacy budget\n",
    "Analysts don’t share anything - PB or Synthetic database\n",
    "Each analyst is only allowed to query their instance of PMW\n",
    "\n",
    "Naive PMW\n",
    "\n",
    "Initialize an instance of PMW with alpha = T and the whole privacy budget \n",
    "All analysts sharing everything - privacy budget and synthetic database\n",
    "Allow every analyst to query that instance of PMW\n",
    "\n",
    "Split PMW\n",
    "\n",
    "Initialize a single instance of PMW for each analyst with alpha = T and the entire privacy budget\n",
    "Split the update steps proportionally to each analyst based on their share of the privacy budget\n",
    "There exists cases where some analysts have more privacy budgets than others - i.e. alice owns 50 percent of the data\n",
    "The difference between split and PMW - is that in Split, everyone shares a synthetic database\n",
    "Inference steps are infamously non-monotonic\n",
    "Allow any analyst to answer from the PMW instance and only allow them to cause an update step if they own any unused update steps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0718e77e574b71e9f7991c7da6831896cfd7281e366db0dbf84de44e8d5f66e5a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
