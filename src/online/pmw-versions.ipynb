{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98fd760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdeb9060",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-58-4930b2d6219e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-4930b2d6219e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def pmw_split(workload, x, T, eps=0.01, k=0, analyst_labels,\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "# Todo: Add code to make epsilon adjustable proportionally. \n",
    "# Todo: Fix divide by zero error (<ipython-input-51-46063bd03d8a>:37: RuntimeWarning: divide by zero encountered in double_scalars delta = 1 / (n * math.log(n, np.e)))\n",
    "\n",
    "def pmw_split(workload, x, T, eps=0.01, k=0, analyst_labels, \n",
    "         show_messages=True, to_return='pd', show_plot=False, show_failure_step=True):\n",
    "    \"\"\"\n",
    "    Implement Private Multiplicative Weights Mechanism (PMW) on a workload of\n",
    "    linear queries. \n",
    "\n",
    "    Algorithm Parameters: \n",
    "    - workload = workload of queries (M x k numpy array)\n",
    "    - x = true database (M x 1 numpy array)\n",
    "    - T = update threshold\n",
    "    - eps = privacy budget\n",
    "    - k = number of update steps PER ANALYST\n",
    "    - analyst_labels = list of analyst names corresponding to each query in the workload\n",
    "    \n",
    "    Output Controls: \n",
    "    - show_messages argument determines whether the function will print information such as \n",
    "    error scale, threshold, update steps used, etc.\n",
    "    - to_return argument determines what the function will return. \n",
    "        - if 'pd', pmw() returns pandas df with test data for each \n",
    "        query in the workload(showing query, d_t_hat, updated, algo_ans, real_ans, \n",
    "        abs_error, rel_error). \n",
    "        - if 'update_count', pmw() returns the update count for the total\n",
    "        amount of queries.\n",
    "    - show_plot - T/F whether the function will display a plot\n",
    "    - show_failure_step - T/F whether function prints what step failure mode is reached\n",
    "    \"\"\" \n",
    "    \n",
    "    update_steps = {}\n",
    "    for analyst in list(set(analyst_labels)): \n",
    "        update_steps[analyst] = k # each analyst starts with k update steps\n",
    "    \n",
    "    # initialize constants\n",
    "    m = x.size  # database len\n",
    "    n = x.sum()\n",
    "    eta = (math.log(m, np.e) ** (1 / 4)) / (math.sqrt(n))\n",
    "    delta = 1 / (n * math.log(n, np.e))\n",
    "    x_norm = x / np.sum(x)\n",
    "    \n",
    "    # initialize synthetic databases at time 0 (prior to any queries)\n",
    "    x_t = np.ones(m) / m\n",
    "    y_t = np.ones(m) / m\n",
    "\n",
    "    # initialize tracker lists to construct pandas dataframe at the end \n",
    "    x_list = [x_t] # create a list of x_t synthetic database at every time step\n",
    "    update_list = []\n",
    "    update_count = 0\n",
    "    pmw_answers = []\n",
    "    update_times = [] # record times that database is updated\n",
    "    d_t_hat_list = []\n",
    "    \n",
    "    def lazy_round():\n",
    "        \"\"\"\n",
    "        \"Lazy Round\" of querying using the stored synthetic database, x_t, in list x_list.\n",
    "        \n",
    "        We call this the lazy round because it is contrasted with the updated step where we update the \n",
    "        sythetic database and answer the query using the real database.\n",
    "        \"\"\"\n",
    "        update_list.append('no')\n",
    "        pmw_answers.append(np.dot(query, x_list[time]))\n",
    "        x_list.append(x_list[time].round(3))\n",
    "    \n",
    "    # inititate first instance of SVT with half the budget and k updates; will be reset in the main loop\n",
    "    SVTtrigger = False \n",
    "    SVTepsilon1 = ((eps/2)/2)\n",
    "    SVTepsilon2 = ((eps/2)/2)\n",
    "    rho = np.random.laplace(loc=0, scale=(1/SVTepsilon1), size=1)[0]\n",
    "    \n",
    "    for time, query in enumerate(workload):\n",
    "        \n",
    "        analyst = analyst_labels[time]\n",
    "        \n",
    "        # Do one round of sparse vector technique \n",
    "        \n",
    "        # Compute noisy answer by adding Laplacian noise\n",
    "        a_t = np.random.laplace(loc=0, scale=(2*k/SVTepsilon2), size=1)[0]\n",
    "        a_t_hat = (np.dot(query, x_norm)*n ) + a_t\n",
    "\n",
    "        # Difference between noisy and maintained histogram answer\n",
    "        d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "        \n",
    "        # Lazy round: use synthetic base to answer the query\n",
    "        if (abs(d_t_hat) <= T + rho):\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            lazy_round()\n",
    "            continue\n",
    "\n",
    "        # update round: update histogram and return noisy answer\n",
    "        else:\n",
    "            #make a new noisy query answer using some of the leftover budget\n",
    "            a_t = np.random.laplace(loc=0, scale=(2*k/eps), size=1)[0]\n",
    "            a_t_hat = (np.dot(query, x_norm)*n ) + a_t\n",
    "            d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            update_times.append(time)\n",
    "            \n",
    "            # step a\n",
    "            if d_t_hat < 0:\n",
    "                r_t = query\n",
    "            else:\n",
    "                r_t = np.ones(m) - query\n",
    "            for i, v in enumerate(y_t):\n",
    "                y_t[i] = x_list[time][i] * math.exp((d_t_hat/(2*n)) * query[i]) * 20 # 20 is the learning rate\n",
    "            \n",
    "            # step b\n",
    "            x_t = y_t / np.sum(y_t)\n",
    "            update_count = update_list.count('yes')\n",
    "            \n",
    "            # if threshold for num updates is reached, just do a lazy round (synthetic database) answer\n",
    "            if update_steps[analyst] == 0: \n",
    "                if show_failure_step:\n",
    "                    print(f'Failure mode reached at query number {time}: {query}')\n",
    "                lazy_round()\n",
    "                \n",
    "            # if there are still update steps that the analyst can use, \n",
    "            # 1. update the synthetic database\n",
    "            # 2. answer the query using the noisy answer from the database itself \n",
    "            else: \n",
    "                x_list.append(x_t.round(3))\n",
    "                update_list.append('yes') # increment number of updates counter\n",
    "                pmw_answers.append(a_t_hat / np.sum(x))\n",
    "                update_steps[analyst] -= 1 # use one of analyst's update steps\n",
    "\n",
    "    update_count = update_list.count('yes')      \n",
    "\n",
    "    # calculate error\n",
    "    real_ans = np.matmul(workload, x_norm)\n",
    "    abs_error = np.abs(pmw_answers - real_ans)\n",
    "    rel_error = np.abs(abs_error / np.where(real_ans == 0, 0.000001,\n",
    "                                                real_ans))\n",
    "    \n",
    "    if show_messages:\n",
    "        np.set_printoptions(suppress=True)\n",
    "        \"\"\"Print inputes/outputs to analyze each query\"\"\"\n",
    "        print(f'Original database: {x}\\n')\n",
    "        print(f'Normalized database: {x_norm}\\n')\n",
    "        print(f'Updated Database = {x_t}\\n')\n",
    "        print(f'Update Count = {update_count}\\n')\n",
    "        print(f'{T=}\\n')\n",
    "        print(f'Error Scale Query Answer= {2*((2*k/eps)**2)}\\n')\n",
    "        print(f'Error Scale SVT= {2*((2*k/SVTepsilon2)**2)}\\n')\n",
    "        print(f'Update Parameter Scale = {eta}\\n')\n",
    "        print(f'{delta=}\\n')\n",
    "        \n",
    "    if show_plot: \n",
    "        plt.title('Error across queries:')\n",
    "        rel_line, = plt.plot(rel_error, label='Relative Error')\n",
    "        abs_line, = plt.plot(abs_error, label='Absolute Error')\n",
    "        for xc in update_times:\n",
    "            plt.axvline(x=xc, color='red', label='Update Times', linestyle='dashed')\n",
    "        plt.legend(handles=[abs_line,rel_line])\n",
    "        plt.xticks(range(0, len(workload), round(len(workload)/5)))\n",
    "    \n",
    "    if to_return == \"pd\":\n",
    "        # hacky fix: remove the first synthetic database to keep length of lists consistent with the\n",
    "        # other lists that comprise of the pandas dataframe\n",
    "        x_list.pop(0).tolist() \n",
    "        d = {\n",
    "            'algo_ans': pmw_answers,\n",
    "            'real_ans': real_ans.tolist(),\n",
    "            'queries': workload.tolist(), \n",
    "            'updated': update_list,\n",
    "            'abs_error': abs_error,               \n",
    "            'rel_error': rel_error,\n",
    "            'synthetic database': x_list,\n",
    "            'analyst': analyst_labels,\n",
    "            'd_t_hat': d_t_hat_list, \n",
    "\n",
    "             }\n",
    "        test_data = pd.DataFrame(data=d)\n",
    "        test_data = test_data.round(3)\n",
    "        return test_data\n",
    "    \n",
    "    # return dictionary of absolute errors\n",
    "    if to_return == \"error\":\n",
    "        d = {'analyst': analyst_labels,\n",
    "             'abs_error': abs_error,               \n",
    "             'rel_error': rel_error,}\n",
    "        data = pd.DataFrame(data=d)\n",
    "        data = data.round(3)\n",
    "        \n",
    "        analyst_error = {}\n",
    "        for analyst in list(set(analyst_labels)):\n",
    "            analyst_error[analyst] = data[data.analyst==analyst]['abs_error'].sum()\n",
    "        return analyst_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4575bffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Original database: [0.2 0.8]\n",
      "\n",
      "Normalized database: [0.2 0.8]\n",
      "\n",
      "Updated Database = [0.5 0.5]\n",
      "\n",
      "Update Count = 0\n",
      "\n",
      "T=5\n",
      "\n",
      "Error Scale Query Answer= 0.0\n",
      "\n",
      "Error Scale SVT= 0.0\n",
      "\n",
      "Update Parameter Scale = 0.9124443057840286\n",
      "\n",
      "delta=inf\n",
      "\n",
      "3\n",
      "Failure mode reached at query number 0: [1 2]\n",
      "Failure mode reached at query number 1: [5 6]\n",
      "Original database: [0.2 0.8]\n",
      "\n",
      "Normalized database: [0.2 0.8]\n",
      "\n",
      "Updated Database = [0.46257015 0.53742985]\n",
      "\n",
      "Update Count = 0\n",
      "\n",
      "T=5\n",
      "\n",
      "Error Scale Query Answer= 0.0\n",
      "\n",
      "Error Scale SVT= 0.0\n",
      "\n",
      "Update Parameter Scale = 0.9124443057840286\n",
      "\n",
      "delta=inf\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-46063bd03d8a>:37: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  delta = 1 / (n * math.log(n, np.e))\n"
     ]
    }
   ],
   "source": [
    "# pmw_independent: write pmw for one person. \n",
    "# create wrapper function called pmw_independent() that takes in the workloads and workload labels. Run PMW for each analyst, separate their workloads based on analysts. \n",
    "\n",
    "database=np.array([.2, .8])\n",
    "\n",
    "def pmw_independent(database):\n",
    "\n",
    "    analyst_labels = ['Alice', 'Bob', 'Alice', 'Bob']\n",
    "    w = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "\n",
    "    indices = {} # k: analyst, v: row indices of queries in the workloads\n",
    "    for i, analyst in enumerate(analyst_labels):\n",
    "        if analyst not in indices.keys(): \n",
    "            indices[analyst] = []\n",
    "        indices[analyst].append(i)\n",
    "\n",
    "    workloads = {} # k: analyst, v: the analyst's workload\n",
    "    for analyst in indices.keys():\n",
    "        workloads[analyst] = w[indices['Alice'], :]\n",
    "\n",
    "    for analyst in workloads.keys():\n",
    "        print(i)\n",
    "        pmw_split(workload=workloads[analyst], x=database, T=5, analyst_labels=[analyst]*len(workloads[analyst]))\n",
    "        \n",
    "pmw_independent(database)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a19abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmw_naive\n",
    "# write version of PMW where analysts can run out of privacy budget if they use too much of others' budgets\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc5cb7",
   "metadata": {},
   "source": [
    "PMW independent\n",
    "\n",
    "Initialize an instance of PMW for each analyst with alpha =T and their share of the privacy budget\n",
    "Analysts don’t share anything - PB or Synthetic database\n",
    "Each analyst is only allowed to query their instance of PMW\n",
    "\n",
    "Naive PMW\n",
    "\n",
    "Initialize an instance of PMW with alpha = T and the whole privacy budget \n",
    "All analysts sharing everything - privacy budget and synthetic database\n",
    "Allow every analyst to query that instance of PMW\n",
    "\n",
    "Split PMW\n",
    "\n",
    "Initialize a single instance of PMW for each analyst with alpha = T and the entire privacy budget\n",
    "Split the update steps proportionally to each analyst based on their share of the privacy budget\n",
    "There exists cases where some analysts have more privacy budgets than others - i.e. alice owns 50 percent of the data\n",
    "The difference between split and PMW - is that in Split, everyone shares a synthetic database\n",
    "Inference steps are infamously non-monotonic\n",
    "Allow any analyst to answer from the PMW instance and only allow them to cause an update step if they own any unused update steps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0718e77e574b71e9f7991c7da6831896cfd7281e366db0dbf84de44e8d5f66e5a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
